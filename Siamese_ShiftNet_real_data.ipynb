{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28df7428-90ab-48de-a7a5-331219f410d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9e48f4-a2d6-4a37-98e3-e333c170ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\accou\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import time\n",
    "from datetime import datetime\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from pynvml import *\n",
    "import kornia\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaba7bc2-a04e-41a0-9f01-8c8242aa9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imports\n",
    "from lanczos import lanczos_2d as lanczos\n",
    "from model.losses import define_loss\n",
    "from utils.helper_functions import get_lr\n",
    "from utils.helper_functions import plot_tensors\n",
    "from utils.helper_functions import plot_tensors_extra_info\n",
    "from utils.dataloader_spot import dataset_spot6\n",
    "\n",
    "# Sesure DataLoader\n",
    "from utils.dataloader import Dataset as dataset\n",
    "\n",
    "# model\n",
    "from model.shiftnet import ShiftNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fcc687-17a6-41e0-9f32-983dfb35b9eb",
   "metadata": {},
   "source": [
    "# Load Model and register/shift/loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ef22ef-5d3c-4cc2-9a2e-0c9d1e8a9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Channels: 1\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "regis_model = ShiftNet(in_channel=1)\n",
    "regis_model = regis_model.train()\n",
    "regis_model = regis_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70753532-1516-4183-a93a-0d92fe25b37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\accou\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.shiftnet import apply_shifts\n",
    "from model.shiftnet import get_shift_loss\n",
    "from model.shiftnet import get_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9e6c1-e922-4d25-8509-d52d4f416f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e64834f-b01f-4361-ab54-aa5e75176407",
   "metadata": {},
   "source": [
    "# Mimic SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626f3358-2733-497e-8263-35a75ab66f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superresolute(lr,factor=4):\n",
    "    return(torch.nn.functional.interpolate(lr, size=None, scale_factor=factor,mode=\"bicubic\"))\n",
    "\n",
    "def shifter(im,shift_factor=0.05):\n",
    "    affinator = torchvision.transforms.Compose([torchvision.transforms.RandomAffine(degrees=0, translate=(shift_factor,shift_factor), scale=None, shear=None,\n",
    "                                    interpolation=torchvision.transforms.InterpolationMode.NEAREST, fill=0, center=None)])\n",
    "    im = affinator(im)\n",
    "    return(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab2127-d456-41b2-8448-b108ac3674f7",
   "metadata": {},
   "source": [
    "# Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1145362-236e-4966-bbae-4480428ebd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Settings\n",
    "# dataloader settings\n",
    "#batch_size = 32\n",
    "epochs = 100\n",
    "lr = 0.001 # try next with 0.00001\n",
    "optimizer = torch.optim.Adam(list(regis_model.parameters()), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',factor=0.1,patience=3,min_lr=0.00000001)\n",
    "\n",
    "# Logging Settings\n",
    "log_freq   =    10 # in it\n",
    "image_freq =  100 # in it\n",
    "val_freq   =  99999999 # in it\n",
    "save_freq  =  99999999 # in epochs\n",
    "wandb_project = \"ShiftNet_sen2\"\n",
    "wandb_entity = \"simon-donike\"\n",
    "\n",
    "# Loss Settings\n",
    "which_loss_func = \"MAE\"  # choose loss: 'MSE', 'MAE','SSIM','PSNR'\n",
    "ssim_window_size = 5\n",
    "\n",
    "# Settings of Shift Data\n",
    "# train on downsampled and manually shifted SPOT6?\n",
    "train_shifted = False\n",
    "shift_pixels = 10 # max shift in pixels\n",
    "shift_factor = round((1/300)*shift_pixels,3) # max shift in factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1852c-75fb-48cd-96da-0dd92994ce01",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c4a7ad9-a8b8-4a28-8617-dd668f36dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataloaders\n",
    "working_directory = \"C:\\\\Users\\\\accou\\\\Documents\\\\GitHub\\\\a-PyTorch-Tutorial-to-Super-Resolution\\\\\"\n",
    "folder_path = \"C:\\\\Users\\\\accou\\\\Documents\\\\thesis\\\\data_v2\\\\\"\n",
    "dataset_file = \"C:\\\\Users\\\\accou\\\\Documents\\\\thesis\\\\data_v2\\\\final_dataset.pkl\"\n",
    "transform = \"histogram_matching\"\n",
    "sen2_tile_train = \"T30UXU\"\n",
    "sen2_tile_test   = \"T30UUU\"\n",
    "sen2_tile_val  = \"all\"\n",
    "location = \"local\"\n",
    "batch_size = 4\n",
    "strat = True # decide wether agricultural areas should be stratified to have more balanced dataset\n",
    "\n",
    "#folder_path,dataset_file,test_train_val=\"train\",transform=\"histogram_matching\",sen2_amount=1,sen2_tile=\"all\",location=\"colab\"):\n",
    "dataset_train = dataset(folder_path,dataset_file,test_train_val=\"train\",transform=transform,sen2_amount=1, location=location,strat=strat)\n",
    "train_loader = DataLoader(dataset_train,batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=4,pin_memory=True,drop_last=True,prefetch_factor=4) # prefetch 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21389349-0527-4afd-a6c7-11a12881e9d0",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb65ed8-cdf8-4828-8a71-fcc18962d5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1q14ozgv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mae</td><td>▂▆▅█▄▁▂▇▆</td></tr><tr><td>mse</td><td>▁▆▇█▄▁▂██</td></tr><tr><td>psnr_loss</td><td>▁▆▇█▅▁▂██</td></tr><tr><td>ssim_loss</td><td>▂▇▅▆▄▁▃█▆</td></tr><tr><td>train_loss</td><td>▅█▃▄▅▁▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>mae</td><td>0.01511</td></tr><tr><td>mse</td><td>0.00076</td></tr><tr><td>psnr_loss</td><td>-31.20064</td></tr><tr><td>ssim_loss</td><td>0.09438</td></tr><tr><td>train_loss</td><td>0.94274</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">20-12-2022_11-51-12</strong>: <a href=\"https://wandb.ai/simon-donike/ShiftNet_sen2/runs/1q14ozgv\" target=\"_blank\">https://wandb.ai/simon-donike/ShiftNet_sen2/runs/1q14ozgv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221220_115113-1q14ozgv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1q14ozgv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\accou\\Documents\\GitHub\\ShiftNet\\wandb\\run-20221220_115337-213ujd6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simon-donike/ShiftNet_sen2/runs/213ujd6n\" target=\"_blank\">20-12-2022_11-53-37</a></strong> to <a href=\"https://wandb.ai/simon-donike/ShiftNet_sen2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss func:  <function l1_loss at 0x00000165B73AD160>\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:43<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:37<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:45<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:40<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:34<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:20<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:17<00:00,  7.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:23<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:23<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##############################################################################| 3427/3427 [07:45<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|###########################################3                                  | 1906/3427 [04:24<03:02,  8.31it/s]"
     ]
    }
   ],
   "source": [
    "# initialize logging\n",
    "run_name = str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "wandb.init(name=run_name,project=wandb_project,entity=wandb_entity)\n",
    "\n",
    "\n",
    "# initialize loss\n",
    "loss_func = define_loss(which_loss_func)\n",
    "\n",
    "lowest_loss = 9999\n",
    "for epoch in range(epochs):\n",
    "    epoch+=1\n",
    "    print(\"Epoch:\",epoch)\n",
    "    it = 0\n",
    "    for lr,hr in tqdm(train_loader,ascii=True):\n",
    "        it+=1\n",
    "        \n",
    "        # PERFORM SR HERE, atm pnly interplation\n",
    "        sr = superresolute(lr,factor=4)\n",
    "        sr,hr = sr.to(device),hr.to(device)\n",
    "        \n",
    "        # calculate predicted thetas\n",
    "        thetas, hr_small, sr_small = get_thetas(hr,sr,regis_model)\n",
    "        # perform shift based on calculated thetas\n",
    "        new_images,thetas = apply_shifts(sr,thetas,regis_model)\n",
    "        # calculate train loss according to defined function\n",
    "        train_loss,hr_loss,new_images_loss = get_shift_loss(new_images,hr,loss_func,sr_small,hr_small)\n",
    "        \n",
    "        # train network\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        # IMAGE LOGGING\n",
    "        if it%image_freq==0 and it!=1:\n",
    "            # create image and turn to WandB format\n",
    "            # wandb_image = plot_tensors(hr,sr,torch.clone(thetas))\n",
    "            #wandb_image = plot_tensors_extra_info(hr,sr,new_images,new_images_loss,hr_loss,torch.clone(thetas))\n",
    "            #wandb.log({\"Training Image\":wandb_image})\n",
    "            plot_tensors_extra_info(a=hr,b=sr,c=new_images,d=hr_loss,e=new_images_loss,f=hr_loss,thetas=torch.clone(thetas))\n",
    "            \n",
    "        # METRICS LOGGING\n",
    "        if it%log_freq==0 and it!=1:\n",
    "            # create loss dict and append image\n",
    "            losses = {\n",
    "            \"epoch\":epoch,\n",
    "            \"train_loss\":train_loss,\n",
    "            \"mae\":torch.nn.functional.l1_loss(new_images,hr),\n",
    "            \"mse\":torch.nn.functional.mse_loss(new_images,hr),\n",
    "            \"ssim_loss\":kornia.losses.ssim_loss(new_images,hr,window_size=ssim_window_size),\n",
    "            \"psnr_loss\":kornia.losses.psnr_loss(new_images,hr,max_val=1.0),\n",
    "            \"lr\":get_lr(optimizer)}\n",
    "            # send to WandB\n",
    "            wandb.log(losses)\n",
    "            # save if loss lower\n",
    "            if train_loss<lowest_loss:\n",
    "                lowest_loss = float(torch.clone(train_loss).detach().cpu().numpy())\n",
    "                torch.save(regis_model,\"checkpoints/regis_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd92943f-a4f1-4d69-9984-0757121c6d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9953f781-2f0c-4a41-9657-753a83db2955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-donike\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\accou\\Documents\\GitHub\\ShiftNet\\wandb\\run-20221220_101804-hgtbkd49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simon-donike/ShiftNet_sen2/runs/hgtbkd49\" target=\"_blank\">20-12-2022_10-18-03</a></strong> to <a href=\"https://wandb.ai/simon-donike/ShiftNet_sen2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss func:  <function l1_loss at 0x000001D6ACE92160>\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################| 428/428 [08:37<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################| 428/428 [08:36<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################| 428/428 [08:36<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################| 428/428 [08:37<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################| 428/428 [08:39<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|################################################################################| 428/428 [08:36<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|######################################################################2         | 376/428 [07:44<01:04,  1.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch)\n\u001b[0;32m     11\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lr,hr \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader,ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     13\u001b[0m     it\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# PERFORM SR HERE, atm pnly interplation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1163\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1165\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1011\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo_env_n6\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    178\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\geo_env_n6\\lib\\threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_name = str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "wandb.init(name=run_name,project=wandb_project,entity=wandb_entity)\n",
    "\n",
    "# initialize loss\n",
    "loss_func = define_loss(which_loss_func)\n",
    "\n",
    "lowest_loss = 9999\n",
    "for epoch in range(epochs):\n",
    "    epoch+=1\n",
    "    print(\"Epoch:\",epoch)\n",
    "    it = 0\n",
    "    for lr,hr in tqdm(train_loader,ascii=True):\n",
    "        it+=1\n",
    "        \n",
    "        # PERFORM SR HERE, atm pnly interplation\n",
    "        sr = superresolute(lr,factor=4)\n",
    "        sr,hr = sr.to(device),hr.to(device)\n",
    "       \n",
    "        # crop image\n",
    "        target_size = 128 # target w & h of image\n",
    "        middle = sr.shape[2] //2 # get middle of tensor\n",
    "        offset = target_size //2 # calculate offset needed from middle of tensor\n",
    "        n_channels = sr.shape[1]\n",
    "        hr_small = torch.clone(hr)[:,0:1,middle-offset:middle+offset,middle-offset: middle+offset] # perform crop and keep only 1 band\n",
    "        sr_small = torch.clone(sr)[:,0:1,middle-offset:middle+offset,middle-offset: middle+offset] # perform crop and keep only 1 band\n",
    "        #print(f'After Cropping: HR shape: {hr_small.shape}, SR shape: {sr_small.shape}')\n",
    "\n",
    "\n",
    "        # rearrange from (B,C,W,H) to (B*3,1,W,H)\n",
    "        hr_small = hr_small.view(-1, 1, 128, 128)\n",
    "        sr_small = sr_small.view(-1, 1, 128, 128)\n",
    "        if hr_small.shape!=sr_small.shape:\n",
    "            print(\"shape mismatch\")\n",
    "        #print(f'After Cropping & rearranging: HR shape: {hr_small.shape}, SR shape: {sr_small.shape}')\n",
    "\n",
    "\n",
    "        ## register_batch via network code\n",
    "        n_views = hr_small.size(1) # get number of views -> amount of images in original, here its 1\n",
    "        thetas = []\n",
    "        for i in range(n_views): # iterate over channels (should be 1 in out case)\n",
    "            theta = regis_model(torch.cat([hr_small[:, i : i + 1], sr_small[:, i : i + 1]], 1)) # send relevant channel to model\n",
    "            thetas.append(theta)\n",
    "        thetas = torch.stack(thetas, 1) # stack return\n",
    "        #print(f'Thetas shape: {thetas.shape}')\n",
    "        thetas = thetas[:, None, :, :].repeat(1, n_channels, 1, 1) # expand back to 3x channels\n",
    "        #print(f'Thetas shape after expanding: {thetas.shape}')\n",
    "\n",
    "        # perform translation\n",
    "        # clone tensors (?)\n",
    "        shifts = torch.clone(thetas)\n",
    "        images = torch.clone(sr)\n",
    "        \n",
    "        # change names for clarity\n",
    "        #shifts=thetas\n",
    "        #images=sr\n",
    "\n",
    "        ## apply_shift code\n",
    "        batch_size, n_views, height, width = images.shape\n",
    "        images = images.view(-1, 1, height, width)\n",
    "        thetas = thetas.view(-1, 2)\n",
    "\n",
    "        #print(f'Apply_shift to input shape: {images.shape}, thetas shape: {thetas.shape}')\n",
    "        # perform translation via built-in function\n",
    "        new_images = regis_model.transform(thetas, images) # error here\n",
    "        #print(f'New Images shifted shape: {new_images.shape}')\n",
    "        # rearrange from (B*C,1,H,W) to (B,3,H,W)\n",
    "        new_images = new_images.view(-1, n_channels, images.size(2), images.size(3))\n",
    "        hr = hr.view(-1, n_channels, images.size(2), images.size(3))\n",
    "        #print(f'HR: {hr.shape} - ShiftNet ouput: {new_images.shape}')\n",
    "\n",
    "        # calculate training loss\n",
    "        # SR has been shifted with regards to HR-GT, therefore we need to calculate loss only over valid pixels\n",
    "        #loss_mask = new_images==0 # get mask where 0\n",
    "        #hr_masked = hr.masked_fill(loss_mask, 0.0) # in hr where mask is 0 with 0 aswell to minimize error\n",
    "        new_images_loss = torch.clone(new_images)[:,:,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "        hr_loss = torch.clone(hr)[:,:,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "        \n",
    "        \n",
    "        #train_loss = loss_func(new_images_loss,hr_loss) # standard loss\n",
    "        \n",
    "        \n",
    "        # Loss as relative decrease in loss as compared to unshifted images\n",
    "        loss_before_shift = loss_func(sr_small,hr_small)\n",
    "        loss_after_shift = loss_func(new_images_loss,hr_loss)\n",
    "        loss_relative = (1/loss_before_shift)*loss_after_shift\n",
    "        train_loss = loss_relative\n",
    "        \n",
    "        # train network\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # VAL calc and Logging\n",
    "        if it%val_freq==0 and it!=0:\n",
    "            # TODO: theta pred and register batches outsourcing\n",
    "            # perform prediction\n",
    "            regis_model = regis_model.eval()\n",
    "            hr_val = next(iter(val_loader))\n",
    "            sr_val = shifter(sr,shift_factor)\n",
    "            hr_val_small = torch.clone(hr_val)[:,0:1,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "            sr_val_small = torch.clone(sr_val)[:,0:1,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "            hr_val_small = hr_small.view(-1, 1, 128, 128)\n",
    "            sr_val_small = sr_small.view(-1, 1, 128, 128)\n",
    "            n_views = hr_val_small.size(1) # get number of views -> amount of images in original, here its 1\n",
    "            thetas_val = []\n",
    "            for i in range(n_views): # iterate over channels (should be 1 in out case)\n",
    "                theta = regis_model(torch.cat([hr_small[:, i : i + 1], sr_small[:, i : i + 1]], 1)) # send relevant channel to model\n",
    "                thetas_val.append(theta)\n",
    "            thetas_val = torch.stack(thetas_val, 1) # stack return\n",
    "            thetas_val = thetas_val[:, None, :, :].repeat(1, n_channels, 1, 1) # expand back to 3x channels\n",
    "           \n",
    "            thetas_pred = regis_model(hr_val)\n",
    "            \n",
    "            hr_val = torch.clone(hr_val)[:,:,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "            pred = torch.clone(pred)[:,:,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "            # calculate losses\n",
    "            val_loss = loss_func(pred,hr_val)\n",
    "            val_losses =  {\n",
    "            \"val_loss\":val_loss,\n",
    "            \"mae_val\":torch.nn.functional.l1_loss(pred,hr_val),\n",
    "            \"mse_val\":torch.nn.functional.mse_loss(pred,hr_val),\n",
    "            \"ssim_loss_val\":kornia.losses.ssim_loss(pred,hr_val,window_size=5),\n",
    "            \"psnr_loss_val\":kornia.losses.psnr_loss(pred,hr_val,max_val=1.0),}\n",
    "            # send to wandb\n",
    "            wandb.log(val_losses)\n",
    "            # put model back to train mode\n",
    "            regis_model = regis_model.train()\n",
    "        \n",
    "        # IMAGE LOGGING\n",
    "        if it%image_freq==0 and it!=1:\n",
    "            # create image and turn to WandB format\n",
    "            # wandb_image = plot_tensors(hr,sr,torch.clone(thetas))\n",
    "            #wandb_image = plot_tensors_extra_info(hr,sr,new_images,new_images_loss,hr_loss,torch.clone(thetas))\n",
    "            #wandb.log({\"Training Image\":wandb_image})\n",
    "            plot_tensors_extra_info(a=hr,b=sr,c=new_images,d=hr_loss,e=new_images_loss,f=hr_loss,thetas=torch.clone(thetas))\n",
    "            \n",
    "        # METRICS LOGGING\n",
    "        if it%log_freq==0 and it!=1:\n",
    "            # create loss dict and append image\n",
    "            losses = {\n",
    "            \"epoch\":epoch,\n",
    "            \"train_loss\":train_loss,\n",
    "            \"mae\":torch.nn.functional.l1_loss(new_images,hr),\n",
    "            \"mse\":torch.nn.functional.mse_loss(new_images,hr),\n",
    "            \"ssim_loss\":kornia.losses.ssim_loss(new_images,hr,window_size=ssim_window_size),\n",
    "            \"psnr_loss\":kornia.losses.psnr_loss(new_images,hr,max_val=1.0),\n",
    "            \"lr\":get_lr(optimizer)}\n",
    "            # send to WandB\n",
    "            wandb.log(losses)\n",
    "            # save if loss lower\n",
    "            if train_loss<lowest_loss:\n",
    "                lowest_loss = float(torch.clone(train_loss).detach().cpu().numpy())\n",
    "                torch.save(regis_model,\"checkpoints/regis_model.pth\")\n",
    "            \n",
    "    # step scheduler\n",
    "    scheduler.step(train_loss)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db7d92-e4f8-4241-a026-7fd981cbe054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08573169-139e-4566-8da7-941f7cf5df50",
   "metadata": {},
   "source": [
    "# Appendix: Sweep Search for best LR and SSIM window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ccf764-9df6-457b-9f71-6982c0f4bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_sweep():\n",
    "    epochs = 15\n",
    "    \n",
    "    wandb.init(project='ShiftNet_sweep')\n",
    "    lr = wandb.config.lr\n",
    "    batch_size = wandb.config.batch_size\n",
    "    which_loss_func =  wandb.config.which_loss_func\n",
    "    #lr = 0.001\n",
    "    #batch_size = 1\n",
    "    #which_loss_func = \"MAE\"\n",
    "    \n",
    "    # initialize dataset and dataloader\n",
    "    dataset_train = dataset_spot6(\"data/train/\")\n",
    "    train_loader = DataLoader(dataset_train,batch_size=batch_size,shuffle=True, num_workers=8,pin_memory=True,drop_last=True,prefetch_factor=32) # prefetch 32\n",
    "    print(\"Dataset Length: \",len(dataset_train))\n",
    "\n",
    "    # initialize loss\n",
    "    loss_func = define_loss(which_loss_func)\n",
    "    \n",
    "    # initialize optim\n",
    "    optimizer = torch.optim.Adam(list(regis_model.parameters()), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch+=1\n",
    "        print(\"Epoch:\",epoch)\n",
    "        it = 0\n",
    "        for hr in tqdm(train_loader,ascii=True):\n",
    "            it+=1\n",
    "            #sr = superresolute(lr) # mimic SR by bicub. interpolation\n",
    "            hr = hr.to(device)\n",
    "            sr = hr.clone() # copy HR as SR\n",
    "\n",
    "            if train_shifted: # if we want synthetic data\n",
    "                #lr = superresolute(hr,factor=0.25) # downsample 300 SPOT6 to 75\n",
    "                #sr = superresolute(lr,factor=0.4)  # artificially super-resolute from 30\n",
    "\n",
    "                sr = shifter(sr,shift_factor) # shift sr image\n",
    "            sr,hr = sr.to(device),hr.to(device)\n",
    "\n",
    "            # crop image\n",
    "            target_size = 128 # target w & h of image\n",
    "            middle = sr.shape[2] //2 # get middle of tensor\n",
    "            offset = target_size //2 # calculate offset needed from middle of tensor\n",
    "            n_channels = sr.shape[1]\n",
    "            hr_small = torch.clone(hr)[:,0:1,middle-offset:middle+offset,middle-offset: middle+offset] # perform crop and keep only 1 band\n",
    "            sr_small = torch.clone(sr)[:,0:1,middle-offset:middle+offset,middle-offset: middle+offset] # perform crop and keep only 1 band\n",
    "            #print(f'After Cropping: HR shape: {hr_small.shape}, SR shape: {sr_small.shape}')\n",
    "\n",
    "\n",
    "            # rearrange from (B,C,W,H) to (B*3,1,W,H)\n",
    "            hr_small = hr_small.view(-1, 1, 128, 128)\n",
    "            sr_small = sr_small.view(-1, 1, 128, 128)\n",
    "            if hr_small.shape!=sr_small.shape:\n",
    "                print(\"shape mismatch\")\n",
    "            #print(f'After Cropping & rearranging: HR shape: {hr_small.shape}, SR shape: {sr_small.shape}')\n",
    "\n",
    "\n",
    "            ## register_batch via network code\n",
    "            n_views = hr_small.size(1) # get number of views -> amount of images in original, here its 1\n",
    "            thetas = []\n",
    "            for i in range(n_views): # iterate over channels (should be 1 in out case)\n",
    "                theta = regis_model(torch.cat([hr_small[:, i : i + 1], sr_small[:, i : i + 1]], 1)) # send relevant channel to model\n",
    "                thetas.append(theta)\n",
    "            thetas = torch.stack(thetas, 1) # stack return\n",
    "            #print(f'Thetas shape: {thetas.shape}')\n",
    "            thetas = thetas[:, None, :, :].repeat(1, n_channels, 1, 1) # expand back to 3x channels\n",
    "            #print(f'Thetas shape after expanding: {thetas.shape}')\n",
    "\n",
    "            # perform translation\n",
    "            # clone tensors (?)\n",
    "            shifts = torch.clone(thetas)\n",
    "            images = torch.clone(sr)\n",
    "\n",
    "            # change names for clarity\n",
    "            #shifts=thetas\n",
    "            #images=sr\n",
    "\n",
    "            ## apply_shift code\n",
    "            batch_size, n_views, height, width = images.shape\n",
    "            images = images.view(-1, 1, height, width)\n",
    "            thetas = thetas.view(-1, 2)\n",
    "\n",
    "            #print(f'Apply_shift to input shape: {images.shape}, thetas shape: {thetas.shape}')\n",
    "            # perform translation via built-in function\n",
    "            new_images = regis_model.transform(thetas, images) # error here\n",
    "            #print(f'New Images shifted shape: {new_images.shape}')\n",
    "            # rearrange from (B*C,1,H,W) to (B,3,H,W)\n",
    "            new_images = new_images.view(-1, n_channels, images.size(2), images.size(3))\n",
    "            hr = hr.view(-1, n_channels, images.size(2), images.size(3))\n",
    "            #print(f'HR: {hr.shape} - ShiftNet ouput: {new_images.shape}')\n",
    "\n",
    "            # calculate training loss\n",
    "            # SR has been shifted with regards to HR-GT, therefore we need to calculate loss only over valid pixels\n",
    "            #loss_mask = new_images==0 # get mask where 0\n",
    "            #hr_masked = hr.masked_fill(loss_mask, 0.0) # in hr where mask is 0 with 0 aswell to minimize error\n",
    "            new_images_loss = torch.clone(new_images)[:,:,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "            hr_loss = torch.clone(hr)[:,:,middle-offset:middle+offset,middle-offset: middle+offset]\n",
    "            #train_loss=nn.functional.l1_loss(new_images_loss,hr_loss) # compute loss\n",
    "            #train_loss = kornia.losses.ssim_loss(new_images_loss,hr_loss,window_size=ssim_window_size)\n",
    "            train_loss = loss_func(new_images_loss,hr_loss)\n",
    "\n",
    "            # train network\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if it%image_freq==0 and it!=1:\n",
    "                # create image and turn to WandB format\n",
    "                # wandb_image = plot_tensors(hr,sr,torch.clone(thetas))\n",
    "                #wandb_image = plot_tensors_extra_info(hr,sr,new_images,new_images_loss,hr_loss,torch.clone(thetas))\n",
    "                #wandb.log({\"Training Image\":wandb_image})\n",
    "                plot_tensors_extra_info(a=hr,b=sr,c=new_images,d=hr_loss,e=new_images_loss,f=hr_loss,thetas=torch.clone(thetas))\n",
    "            if it%log_freq==0 and it!=1:\n",
    "                # create loss dict and append image\n",
    "                losses = {\n",
    "                \"epoch\":epoch,\n",
    "                \"train_loss\":train_loss,\n",
    "                \"mae\":torch.nn.functional.l1_loss(new_images,hr),\n",
    "                \"mse\":torch.nn.functional.mse_loss(new_images,hr),\n",
    "                \"ssim_loss\":kornia.losses.ssim_loss(new_images,hr,window_size=ssim_window_size),\n",
    "                \"psnr_loss\":kornia.losses.psnr_loss(new_images,hr,max_val=1.0),}\n",
    "\n",
    "                # send to WandB\n",
    "                wandb.log(losses)\n",
    "                \n",
    "                # save if best result yet\n",
    "\n",
    "        \n",
    "        #scheduler.step(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8500fe-7d55-4a06-8ba4-84585ddc8c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kpdveo60\n",
      "Sweep URL: https://wandb.ai/simon-donike/ShiftNet_sweep/sweeps/kpdveo60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e2to4hi5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_loss_func: MAE\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-donike\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\accou\\Documents\\GitHub\\ShiftNet\\wandb\\run-20221219_175553-e2to4hi5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simon-donike/ShiftNet_sweep/runs/e2to4hi5\" target=\"_blank\">dauntless-sweep-1</a></strong> to <a href=\"https://wandb.ai/simon-donike/ShiftNet_sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/simon-donike/ShiftNet_sweep/sweeps/kpdveo60\" target=\"_blank\">https://wandb.ai/simon-donike/ShiftNet_sweep/sweeps/kpdveo60</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:  10000\n",
      "Loss func:  <function l1_loss at 0x000001B088940160>\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:23<00:00, 38.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:16<00:00, 38.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:17<00:00, 38.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:15<00:00, 39.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:13<00:00, 39.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:14<00:00, 39.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:13<00:00, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:14<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:14<00:00, 39.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:14<00:00, 39.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:14<00:00, 39.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:14<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:13<00:00, 39.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:15<00:00, 39.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:13<00:00, 39.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>mae</td><td>▅▅▅▄▆▄▄▄▅█▅▄▅▄▇▄▅▅▂▆▅▇▅▄▅▄▃▄▆▁█▅▆▆▅▄▆▆▄▅</td></tr><tr><td>mse</td><td>▄▄▄▃▅▃▂▃▄█▃▂▃▃▆▃▄▄▁▄▄▇▄▃▄▂▂▃▄▁▇▄▅▅▄▃▅▅▂▃</td></tr><tr><td>psnr_loss</td><td>███▇█▇▇▇███▇██████▇████▇█▇▇██▁████████▇█</td></tr><tr><td>ssim_loss</td><td>▅▅▅▅▇▄▅▄▅█▅▄▅▄▇▄▄▅▂▅▅▇▅▄▅▄▃▄▅▁▇▄▆▆▅▄▅▆▄▅</td></tr><tr><td>train_loss</td><td>▄▄▅▄█▄▃▄▄█▄▄▄▃▇▃▃▅▂▄▅▇▄▄▅▃▃▃▄▁▇▄▆▇▄▃▄▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>mae</td><td>0.01669</td></tr><tr><td>mse</td><td>0.00087</td></tr><tr><td>psnr_loss</td><td>-30.60095</td></tr><tr><td>ssim_loss</td><td>0.09286</td></tr><tr><td>train_loss</td><td>0.01293</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dauntless-sweep-1</strong>: <a href=\"https://wandb.ai/simon-donike/ShiftNet_sweep/runs/e2to4hi5\" target=\"_blank\">https://wandb.ai/simon-donike/ShiftNet_sweep/runs/e2to4hi5</a><br/>Synced 6 W&B file(s), 150 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221219_175553-e2to4hi5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ufkeu1fq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \twhich_loss_func: MAE\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\accou\\Documents\\GitHub\\ShiftNet\\wandb\\run-20221219_185955-ufkeu1fq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/simon-donike/ShiftNet_sweep/runs/ufkeu1fq\" target=\"_blank\">soft-sweep-2</a></strong> to <a href=\"https://wandb.ai/simon-donike/ShiftNet_sweep\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/simon-donike/ShiftNet_sweep/sweeps/kpdveo60\" target=\"_blank\">https://wandb.ai/simon-donike/ShiftNet_sweep/sweeps/kpdveo60</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length:  10000\n",
      "Loss func:  <function l1_loss at 0x000001B088940160>\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:12<00:00, 39.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:14<00:00, 39.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:12<00:00, 39.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:15<00:00, 39.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:13<00:00, 39.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:15<00:00, 39.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:12<00:00, 39.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:13<00:00, 39.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|############################################################################| 10000/10000 [04:18<00:00, 38.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|#########################################################1                   | 7422/10000 [03:16<01:11, 36.10it/s]"
     ]
    }
   ],
   "source": [
    "run_name = str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "sweep_configuration = {\n",
    "    'method': 'grid',\n",
    "    'name': 'sweep',\n",
    "    'metric': {'goal': 'minimize', 'name': 'train_loss'},\n",
    "    'parameters': \n",
    "    {\n",
    "        'lr': {\"values\":[0.001,0.0001]},\n",
    "        'which_loss_func':{\"values\":[\"MSE\"]},  #\"MSE\",\"SSIM\",\"PSNR\"\n",
    "        'batch_size':{\"values\":[1,4,16]},\n",
    "     }\n",
    "}\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(entity=\"simon-donike\",sweep=sweep_configuration, project='ShiftNet_sweep')\n",
    "wandb.agent(sweep_id, function=train_sweep, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154332c6-36c2-4a4d-a3cf-12b4685f69cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8f003-3763-4fa0-821e-d0de6e300c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
